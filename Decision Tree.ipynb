{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd75e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f32c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\\\Datasets\\\\pulsar_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ac2399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121.156250</td>\n",
       "      <td>48.372971</td>\n",
       "      <td>0.375485</td>\n",
       "      <td>-0.013165</td>\n",
       "      <td>3.168896</td>\n",
       "      <td>18.399367</td>\n",
       "      <td>7.449874</td>\n",
       "      <td>65.159298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.968750</td>\n",
       "      <td>36.175557</td>\n",
       "      <td>0.712898</td>\n",
       "      <td>3.388719</td>\n",
       "      <td>2.399666</td>\n",
       "      <td>17.570997</td>\n",
       "      <td>9.414652</td>\n",
       "      <td>102.722975</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.585938</td>\n",
       "      <td>53.229534</td>\n",
       "      <td>0.133408</td>\n",
       "      <td>-0.297242</td>\n",
       "      <td>2.743311</td>\n",
       "      <td>22.362553</td>\n",
       "      <td>8.508364</td>\n",
       "      <td>74.031324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.398438</td>\n",
       "      <td>48.865942</td>\n",
       "      <td>-0.215989</td>\n",
       "      <td>-0.171294</td>\n",
       "      <td>17.471572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.958066</td>\n",
       "      <td>7.197842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.804688</td>\n",
       "      <td>36.117659</td>\n",
       "      <td>0.825013</td>\n",
       "      <td>3.274125</td>\n",
       "      <td>2.790134</td>\n",
       "      <td>20.618009</td>\n",
       "      <td>8.405008</td>\n",
       "      <td>76.291128</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean of the integrated profile  \\\n",
       "0                       121.156250   \n",
       "1                        76.968750   \n",
       "2                       130.585938   \n",
       "3                       156.398438   \n",
       "4                        84.804688   \n",
       "\n",
       "    Standard deviation of the integrated profile  \\\n",
       "0                                      48.372971   \n",
       "1                                      36.175557   \n",
       "2                                      53.229534   \n",
       "3                                      48.865942   \n",
       "4                                      36.117659   \n",
       "\n",
       "    Excess kurtosis of the integrated profile  \\\n",
       "0                                    0.375485   \n",
       "1                                    0.712898   \n",
       "2                                    0.133408   \n",
       "3                                   -0.215989   \n",
       "4                                    0.825013   \n",
       "\n",
       "    Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "0                            -0.013165                   3.168896   \n",
       "1                             3.388719                   2.399666   \n",
       "2                            -0.297242                   2.743311   \n",
       "3                            -0.171294                  17.471572   \n",
       "4                             3.274125                   2.790134   \n",
       "\n",
       "    Standard deviation of the DM-SNR curve  \\\n",
       "0                                18.399367   \n",
       "1                                17.570997   \n",
       "2                                22.362553   \n",
       "3                                      NaN   \n",
       "4                                20.618009   \n",
       "\n",
       "    Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \\\n",
       "0                              7.449874                      65.159298   \n",
       "1                              9.414652                     102.722975   \n",
       "2                              8.508364                      74.031324   \n",
       "3                              2.958066                       7.197842   \n",
       "4                              8.405008                      76.291128   \n",
       "\n",
       "   target_class  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ef9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isna().sum()\n",
    "df.rename(columns={' Mean of the integrated profile': '1', ' Standard deviation of the integrated profile': '2',\n",
    "                  ' Excess kurtosis of the integrated profile': '3', ' Skewness of the integrated profile': '4', ' Mean of the DM-SNR curve': '5', ' Standard deviation of the DM-SNR curve': '6',\n",
    "                  ' Excess kurtosis of the DM-SNR curve': '7', ' Skewness of the DM-SNR curve': '8'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8829054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d1ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "707f73ed",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Classification and Regression Trees (CART)  is an acronym introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems. The representation of the CART model is a binary tree, with a node representing a single input variable (X) and a split point on that variable. The leaf nodes (also called terminal nodes) of the tree contain an output variable (y) which is used to make a prediction.\n",
    "\n",
    "To create the binary tree, involves dividing up the input space using a greedy approach called recursive binary splitting. For that, we simply line up all the values and try different split points, testing them using a cost function. The split with the best cost (lowest cost because we minimize cost) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function. Splitting will continue until nodes contain a minimum number of training examples or a maximum tree depth is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b52ff14",
   "metadata": {},
   "source": [
    "#  Gini Index\n",
    "\n",
    "The Gini index will be the cost function used in this project. A split in the node will take in one input attribute and its corresponding split point. The Gini dcore will then give an idea of how good that split was by how mixed the classes are in the two resulting groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split result in a Gini score of 0.5 (For binary classification) showing that there is equal distribution of elements across some classes. The Gini index can be calculated as:\n",
    "    G =p(1) ∗ (1−p(1)) + p(2) ∗ (1−p(2)) or 1 - P(1)^2 + P(2)^2\n",
    "where p(1) and p(2) is the probability  of chosing that class in a node. To find the Gini impurity \n",
    "of the whole split we use:\n",
    "    N(1)/n * G(1) + N(2)/n * G(2)\n",
    "where N(1) and N(2) are the number of elements in that node, n is total number of elements between the two nodes, and G(1) and G(2) are the corresponding Gini scores for the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dea1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(split):\n",
    "    # split is list of children nodes split from 1 parent \n",
    "    total_n = sum([len(node) for node in split])\n",
    "    total_gini = 0\n",
    "    \n",
    "    for node in split:\n",
    "        n = len(node)\n",
    "        \n",
    "        if n ==0:\n",
    "            continue\n",
    "            \n",
    "        score = 0\n",
    "        \n",
    "        for class_val in node.target_class.unique():\n",
    "            p = node['target_class'].value_counts()[class_val] / n\n",
    "            score += p**2\n",
    "        \n",
    "        total_gini += (1.0 - score) * (n / total_n)\n",
    "        \n",
    "    return total_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b8300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea16edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(index, value, dataset):\n",
    "    '''\n",
    "    index = attribute being evaluated\n",
    "    value = point of split in index\n",
    "    '''\n",
    "    temp = dataset[index] > value\n",
    "    right = dataset[temp]\n",
    "    left = dataset[~temp]    \n",
    "    return (left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e352889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f471c018",
   "metadata": {},
   "source": [
    "# Evaluating Splits\n",
    "\n",
    "Because this model is using an an exhaustive and greedy algorithm, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make. To do this, we will use a dictionary to represent a given node in the tree, in it we will store the index of a chosen attribute, the value of that attribute by which to split, and the two groups of data split by the chosen split point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ad0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(dataset):\n",
    "    \n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    \n",
    "    for name, values in dataset.items():\n",
    "        \n",
    "        if name == 'target_class':\n",
    "            break \n",
    "            \n",
    "        for value in values:\n",
    "            groups = split(name, value, dataset)\n",
    "            gini = Gini(groups)\n",
    "            \n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = name, value, gini, groups\n",
    "                \n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a9380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38ae1a1",
   "metadata": {},
   "source": [
    "# Building the Tree\n",
    "\n",
    "Now that we can find the best split point for a node, we can focus on building the tree. Terminal nodes are nodes to stop growing, it can be decided through depth or the number of rows that the node is responsible for in the training dataset. Depth is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop adding new nodes to prevent over fitting. Minimum Node Records, is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting and adding new nodes. The last terminal condition is if a node contains only 1 class in which case it acheived its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2d7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_terminal(group):\n",
    "    return int(group['target_class'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4d61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if left.empty or right.empty:\n",
    "        node['left'] = node['right'] = to_terminal(pd.concat([left, right]))\n",
    "        return\n",
    "    \n",
    "     # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = find_split(left)\n",
    "        create_split(node['left'], max_depth, min_size, depth+1)\n",
    "        \n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = find_split(right)\n",
    "        create_split(node['right'], max_depth, min_size, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c3f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, max_depth, min_size):\n",
    "    root = find_split(data)\n",
    "    create_split(root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb4edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, x):\n",
    "    if x[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], x)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "                return predict(node['right'], x)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be49fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    tree = build_tree(train, max_depth, min_size)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbdbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2418cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    copy = dataset.copy()\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        temp = list()\n",
    "        while len(temp) < fold_size:\n",
    "            ind = randrange(len(copy))\n",
    "            temp.append(copy.iloc[ind])\n",
    "            copy.drop(copy.index[ind])\n",
    "        dataset_split.append(pd.DataFrame(temp))\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1136582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    correct = 0\n",
    "    for i in range(len(yhat)):\n",
    "        if y[i] == yhat[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(y)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d363a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = kfold(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for i in range(len(folds)):\n",
    "        train_set = list(folds)\n",
    "        train_set.pop(i)\n",
    "        train_set = pd.concat(i for i in train_set)\n",
    "        test_set = list()\n",
    "        for j in range(len(folds[i])):\n",
    "            copy = folds[i].iloc[j].copy()\n",
    "            test_set.append(copy)\n",
    "            copy[-1] = None\n",
    "            predicted = algorithm(train_set, test_set, *args)\n",
    "            actual = [folds[i]['target_class'].iloc[k] for k in range(len(folds[i]))]\n",
    "        acc = accuracy(actual, predicted)\n",
    "        scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47432b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff752943",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "scores = evaluate_algorithm(test, decision_tree, n_folds, max_depth, min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77bd31d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [100.0, 95.0, 90.0, 95.0, 90.0]\n",
      "Mean Accuracy: 94.000%\n"
     ]
    }
   ],
   "source": [
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112531e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c74291c",
   "metadata": {},
   "source": [
    "# Information Gain \n",
    "\n",
    "In most cases, the Gini index will be the best criterion for finding the split point of a node. However, i wanted to cover another option that could be used. Information Gain uses entropy to decide to to make a split, being calculated as:\n",
    "   Information Gain = 1 - entropy\n",
    "with the entropy being:\n",
    "    -sum(P[i] * log(P[i]))\n",
    "Where the pi is the probability of randomly picking one element of that specific class. In most cases both algorithms give the same results and since the Gini algorithm is computationally cheaper, its usally the better option. One benifit of entropy though, is that is has been proven to be stronger than the Gini Index on datasets that are heavily unbalanced."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
