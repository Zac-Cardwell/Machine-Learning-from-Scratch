{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448ebf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5ebdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, kernel_size, in_chanels, out_chanels):\n",
    "        self.kernel = np.random.rand(out_chanels, in_chanels, kernel_size[0], kernel_size[1])\n",
    "        self.stride = 1\n",
    "        self.inputz = None\n",
    "        \n",
    "    def corr(self, x, k):\n",
    "        h, w = k.shape\n",
    "        stride = self.stride\n",
    "        height = (x.shape[0] - h + stride)/stride\n",
    "        width = (x.shape[1] - w + stride)/stride\n",
    "        Y = np.zeros((int(height), int(width)))\n",
    "        \n",
    "        for i in range(Y.shape[0]):\n",
    "            for j in range(Y.shape[1]):\n",
    "                Y[i, j] = (x[int(i*stride):int(i*stride) + h, int(j*stride):int(j*stride) + w] * k).sum() \n",
    "        return Y\n",
    "    \n",
    "    def corr2d_multi_in(self,X, K, pad):\n",
    "        return sum(self.corr(np.pad(x, pad), k)for x, k in zip(X, K))\n",
    "    \n",
    "    def back(self, X, der, pad=0, stride=1):\n",
    "        return np.stack([self.back_corr(x, der, pad, stride) for x in X], 0)\n",
    "\n",
    "    def back_corr(self, X, der, pad, stride):\n",
    "        return sum(self.corr(np.pad(x, pad), d)for x, d in zip(X, der))\n",
    "    \n",
    "    def foward(self, X, pad=0, stride=0):\n",
    "        if stride:\n",
    "            self.stride = stride\n",
    "            \n",
    "        self.inputz = X\n",
    "        return np.stack([self.corr2d_multi_in(X, k, pad) for k in self.kernel], 0)\n",
    "    \n",
    "    def backward(self,der, lr, pad):\n",
    "        X = np.asarray(self.inputz)\n",
    "        \n",
    "        dk = self.back(np.expand_dims(X, axis=1), der, pad=0, stride=1)\n",
    "        dx = np.stack([self.corr2d_multi_in(der, k, pad) for k in np.flip(np.flip(self.kernel, 2), 1).transpose(1,0,2,3)], 0)\n",
    "        \n",
    "        for k in self.kernel:\n",
    "            k = k - (-lr*dk)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f0c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pooling:\n",
    "    def __init__(self, size, stride):\n",
    "        self.h, self.w = size\n",
    "        self.stride = stride\n",
    "        self.gradient = None\n",
    "        self.Y = None\n",
    "        self.input = None\n",
    "        \n",
    "        \n",
    "    def fpass(self, x):\n",
    "        height = (x.shape[0] - self.h + self.stride)/self.stride\n",
    "        width = (x.shape[1] - self.w + self.stride)/self.stride\n",
    "        Y = np.zeros((int(height), int(width)))\n",
    "        for i in range(Y.shape[0]):\n",
    "            for j in range(Y.shape[1]):\n",
    "                Y[i, j] = x[i*self.stride:i*self.stride + self.h, j*self.stride:j*self.stride + self.w].max()\n",
    "        return Y\n",
    "    \n",
    "    \n",
    "    def find_gradient(self, x, Y, der):\n",
    "        grad = np.zeros((x.shape[0], x.shape[1]))\n",
    "        for i in range(Y.shape[0]):\n",
    "            for j in range(Y.shape[1]):\n",
    "                a, b = np.where(x[i*self.stride:i*self.stride + self.h, j*self.stride:j*self.stride + self.w] == Y[i, j])\n",
    "                grad[i*self.stride+a[0], j*self.stride+b[0]] = der[i, j]\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    def foward(self, X):\n",
    "        self.input = X\n",
    "        self.Y =  np.stack([self.fpass(x) for x in X], 0)\n",
    "        return self.Y\n",
    "        \n",
    "        \n",
    "    def backward(self, Der):\n",
    "        X = self.input\n",
    "        self.gradient =  np.stack([self.find_gradient(x, y, der) for x, y, der in zip(X, self.Y, Der)], 0)\n",
    "        return self.gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db62beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten_layer:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "        \n",
    "    def flatten(self, X):\n",
    "        self.shape = X.shape\n",
    "        return np.expand_dims(X.flatten(), 0)\n",
    "    \n",
    "    def back(self, X):\n",
    "        return X.resize(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2728c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCL:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(1,output_size)\n",
    "        self.input = None\n",
    "    \n",
    "    def foward(self,x):\n",
    "        self.input = x\n",
    "        y = x.dot(self.weights) + self.bias\n",
    "        return y\n",
    "    \n",
    "    def backward(self, der, lr):\n",
    "        x = self.input\n",
    "        dw = x.T.dot(der)\n",
    "        db = der\n",
    "        dx = der.dot(self.weights.T)\n",
    "        \n",
    "        self.weights = self.weights - (-lr*dw)\n",
    "        self.bias = self.bias - (-lr*db)\n",
    "        \n",
    "        #print(f\"dw: {dw.shape} dx: {dx.shape}  db: {db.shape}\")\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b2bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def foward(self, x):\n",
    "        self.input=x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def back(self):\n",
    "        return 1 - np.tanh(self.input)**2\n",
    "    \n",
    "def softmax_cross_entropy_loss(yhat, y):\n",
    "    yhat -= np.max(yhat, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(yhat)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    logprobs = np.zeros([len(yhat),1])\n",
    "    for r in range(len(yhat)): # For each element in the batch\n",
    "        scale_factor = 1 / float(np.count_nonzero(y[r, :]))\n",
    "        for c in range(len(y[r,:])): # For each class \n",
    "            if y[r,c] != 0:  # Positive classes\n",
    "                logprobs[r] += -np.log(probs[r,c]) * y[r,c] * scale_factor # We sum the loss per class for each element of the batch\n",
    "    data_loss = np.sum(logprobs) / len(yhat)    \n",
    "    return probs, data_loss\n",
    "\n",
    "def cross_backward(y, yhat, probs):\n",
    "    delta = probs   # If the class label is 0, the gradient is equal to probs\n",
    "    labels = y\n",
    "    for r in range(len(yhat)):  # For each element in the batch\n",
    "        scale_factor = 1 / float(np.count_nonzero(labels[r, :]))\n",
    "        for c in range(len(labels[r,:])):  # For each class\n",
    "            if labels[r, c] != 0:  # If positive class\n",
    "                delta[r, c] = scale_factor * (delta[r, c] - 1) + (1 - scale_factor) * delta[r, c]\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28887c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.con1 = Conv([2,2],1,4)\n",
    "        self.pool1 = max_pooling([2,2], 2)\n",
    "        self.con2 = Conv([3,3],4,2)\n",
    "        self.con3 = Conv([2,2],2,1)\n",
    "        self.pool2 = max_pooling([2,2], 2)\n",
    "        self.flat = flatten_layer()\n",
    "        self.aclayer1 = tanh()\n",
    "        self.layer1 = FCL(25,15)\n",
    "        self.aclayer2 = tanh()\n",
    "        self.layer2 = FCL(15,10)\n",
    "        \n",
    "    def foward(self, X, Y):\n",
    "        out1 = self.con1.foward(X, stride=1, pad=0)\n",
    "        #print(out1.shape)\n",
    "        out2 = self.pool1.foward(out1)\n",
    "        #print(out2.shape)\n",
    "        out3 = self.con2.foward(out2, stride=1, pad=0)\n",
    "        #print(out3.shape)\n",
    "        out4 = self.con3.foward(out3, stride=1, pad=0)\n",
    "        out5 = self.pool2.foward(out4)\n",
    "        out6 = self.flat.flatten(out5)\n",
    "        z1 = self.aclayer1.foward(out6)\n",
    "        out7 = self.layer1.foward(z1)\n",
    "        z2 = self.aclayer2.foward(out7)\n",
    "        self.final = self.layer2.foward(z2)\n",
    "        probs, loss = softmax_cross_entropy_loss(self.final, Y[0])\n",
    "        return probs, loss\n",
    "    \n",
    "    def backward(self, probs, lr, Y):\n",
    "        dl = cross_backward(Y[0], self.final, probs)\n",
    "        d1 = self.layer2.backward(dl, lr)\n",
    "        dz1 = d1 * self.aclayer2.back()\n",
    "        d2 = self.layer1.backward(dz1, lr)\n",
    "        dz2 = d2 * self.aclayer1.back()\n",
    "        self.flat.back(dz2)\n",
    "        d3 = self.pool2.backward(dz2)\n",
    "        d4 = self.con3.backward(d3, lr, 1)\n",
    "        #print(d4.shape)\n",
    "        d5 = self.con2.backward(d4, lr, 2)\n",
    "        #print(d5.shape)\n",
    "        d6 = self.pool1.backward(d5)\n",
    "        #print(d6.shape) \n",
    "        d7 = self.con1.backward(d6, lr, 1)\n",
    "        #print(d7.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ee9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 23:52:32.381506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_image, train_label), (test_image, test_label) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = train_image.astype('float32')/255\n",
    "test_image = test_image.astype('float32')/255\n",
    "train_image = np.expand_dims(train_image, 1)\n",
    "test_image = np.expand_dims(test_image, 1)\n",
    "\n",
    "\n",
    "train_label = to_categorical(train_label)\n",
    "test_label = to_categorical(test_label)\n",
    "train_label = np.expand_dims(train_label, 1)\n",
    "test_label = np.expand_dims(test_label, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e8bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b010408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dffc9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/60000 [00:00<18:42, 53.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.4391551911008302  avg: 6.4957609627493635  \n",
      "ry = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.03209179  0.13208643  0.04602457  0.24766555  0.01973615 -0.95295289\n",
      "   0.07779267  0.23132864  0.03498796  0.13123914]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6024/60000 [01:40<15:07, 59.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.717721247854944  avg: 2.643509091455922  \n",
      "ry = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.02428925  0.14092945  0.0450502   0.254894    0.01884076  0.04241727\n",
      "   0.07854595  0.25059993  0.02660291 -0.88216972]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12014/60000 [03:19<13:46, 58.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.0216271620613275  avg: 2.668580211151948  \n",
      "ry = [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[-0.97859022  0.15154042  0.04394974  0.28377421  0.01792378  0.03801267\n",
      "   0.07289356  0.25408756  0.02002556  0.09638272]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 18010/60000 [05:00<12:05, 57.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0999386592922265  avg: 2.706014370103567  \n",
      "ry = [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.01625186  0.10906076  0.03402533  0.3328915   0.01356172  0.0344398\n",
      "   0.06302046  0.2879228   0.01646518 -0.90763941]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24016/60000 [06:40<10:19, 58.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.479364503454057  avg: 2.7564159720979373  \n",
      "ry = [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.01551245 -0.88586767  0.03049533  0.33675119  0.0119102   0.030827\n",
      "   0.06294727  0.30894809  0.01450784  0.07396831]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 30020/60000 [08:21<08:31, 58.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0918883049248718  avg: 2.8051088113768383  \n",
      "ry = [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]] \n",
      "yhat = [[ 0.01252615  0.09264876 -0.97293978  0.37488261  0.00816495  0.02521326\n",
      "   0.0531298   0.33558221  0.01268448  0.05810757]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 36013/60000 [10:01<06:59, 57.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.8463733964907005  avg: 2.862528915429319  \n",
      "ry = [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.0097712   0.0716068   0.02135705  0.46773267  0.00634341  0.01692484\n",
      "   0.04661529  0.2985306  -0.99010322  0.05122138]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 42021/60000 [11:42<05:07, 58.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.251908836012375  avg: 2.940383338991663  \n",
      "ry = [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.00699692  0.05398883  0.01423703  0.59385138  0.00490734  0.01219813\n",
      "   0.03448011  0.23477896  0.00653922 -0.96197791]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 48011/60000 [13:22<03:28, 57.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8317429488949826  avg: 3.036695803776055  \n",
      "ry = [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]] \n",
      "yhat = [[ 0.00463431 -0.96571539  0.00745413  0.73542402  0.00299532  0.00748768\n",
      "   0.0233591   0.16013422  0.0039958   0.0202308 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 54010/60000 [15:02<01:43, 57.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1454346455774124  avg: 3.170567044068309  \n",
      "ry = [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]] \n",
      "yhat = [[ 0.00253023  0.01502502  0.00320764  0.8646464   0.00139159  0.00409695\n",
      "   0.01293667 -0.91686976  0.00185789  0.01117736]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [16:43<00:00, 59.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3369736824896012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "avg=0\n",
    "labels = list()\n",
    "t=0\n",
    "for i in tqdm(range(len(train_image))):\n",
    "    probs, loss = test.foward(train_image[i], np.array([train_label[i]]))\n",
    "    avg += loss\n",
    "    labels.append(probs)\n",
    "    \n",
    "    if i % 500 == 1:\n",
    "        test.backward(probs, .005, train_label[t:i])\n",
    "        labels = list()\n",
    "        t = i\n",
    "        \n",
    "    if i % 6000 == 1:\n",
    "        print(f\"loss: {loss}  avg: {avg/i}  \\ny = {train_label[i]} \\nyhat = {probs}\")\n",
    "        \n",
    "print(avg/len(train_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
